# -*- coding: utf-8 -*-
"""Garbage_Classification_Using_CNN&AlexNet(v2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ZBpAdi_VRMr6tJ4BhnFpDC5yykRUKWF

# **Garbage Classification using AlexNet**
In this notebook, we'll implement a garbage classification system using the AlexNet CNN architecture. We'll be working with a dataset containing images of 6 different garbage categories: cardboard, glass, metal, paper, plastic, and trash.

# **1. Setup and Data Import**
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
import kagglehub
garbage_classification_path = kagglehub.dataset_download('asdasdasasdas/garbage-classification')
print('Data source import complete.')

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

"""# **2. Configuration Parameters**"""

# Constants
train_dir = '/kaggle/input/garbage-classification/Garbage classification/Garbage classification'
labels = ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']

IMAGE_SIZE = (384, 512)  # Image dimensions
BATCH_SIZE = 4  # Reduced due to larger image size
EPOCHS = 30  # Maximum training epochs

"""# **3. Data Exploration**

"""

# Display sample images from each class
plt.figure(figsize=(30, 14))
for i in range(6):
    directory = os.path.join(train_dir, labels[i])
    for j in range(10):
        path = os.path.join(directory, os.listdir(directory)[j])
        img = mpimg.imread(path)
        plt.subplot(6, 10, i*10 + j + 1)
        plt.imshow(img)
        if j == 0:
            plt.ylabel(labels[i], fontsize=20)
plt.setp(plt.gcf().get_axes(), xticks=[], yticks=[])
plt.tight_layout()
plt.show()

"""# **4. Data Preparation**"""

# Create data generators with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=15,
    zoom_range=0.1,
    width_shift_range=0.15,
    height_shift_range=0.15,
    shear_range=0.1,
    horizontal_flip=True,
    vertical_flip=True,
    fill_mode="nearest",
    validation_split=0.2
)

# Create data generators function
def create_generators():
    train_generator = train_datagen.flow_from_directory(
        train_dir, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,
        class_mode='sparse', subset='training', shuffle=True
    )

    validation_generator = train_datagen.flow_from_directory(
        train_dir, target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,
        class_mode='sparse', subset='validation', shuffle=False
    )

    return train_generator, validation_generator

"""# **5. AlexNet Model Architecture**"""

# AlexNet model creation function
def create_alexnet(dropout_rate=0.5, learning_rate=0.0001):
    model = Sequential([
        # Layer 1: Conv -> ReLU -> Pool
        Conv2D(96, (11, 11), strides=(4, 4), activation='relu', padding='same', input_shape=(*IMAGE_SIZE, 3)),
        MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
        BatchNormalization(),

        # Layer 2: Conv -> ReLU -> Pool
        Conv2D(256, (5, 5), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
        BatchNormalization(),

        # Layer 3: Conv -> ReLU
        Conv2D(384, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),

        # Layer 4: Conv -> ReLU
        Conv2D(384, (3, 3), padding='same', activation='relu'),
        BatchNormalization(),

        # Layer 5: Conv -> ReLU -> Pool
        Conv2D(256, (3, 3), padding='same', activation='relu'),
        MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),
        BatchNormalization(),

        # Flatten
        Flatten(),

        # FC Layers
        Dense(4096, activation='relu'),
        Dropout(dropout_rate),

        Dense(4096, activation='relu'),
        Dropout(dropout_rate),

        # Output Layer
        Dense(6, activation='softmax')  # 6 classes for garbage classification
    ])

    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])

    # Display model summary
    model.summary()

    return model

"""# **6. Training History Visualization**"""

# Function to plot training history
def plot_training_history(history, title):
    plt.figure(figsize=(12, 5))

    # Plot accuracy
    plt.subplot(1, 2, 1)
    plt.plot(history.history['accuracy'], label='Training Accuracy')
    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
    plt.title(f'Accuracy - {title}')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()

    # Plot loss
    plt.subplot(1, 2, 2)
    plt.plot(history.history['loss'], label='Training Loss')
    plt.plot(history.history['val_loss'], label='Validation Loss')
    plt.title(f'Loss - {title}')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()

    plt.tight_layout()
    plt.show()

"""# **7. Model Evaluation Function**"""

# Evaluate model function
def evaluate_model(model, validation_generator, title):
    print(f"\n--- Evaluating {title} ---")

    # Get predictions
    val_preds = model.predict(validation_generator)
    y_pred = np.argmax(val_preds, axis=1)
    y_true = validation_generator.classes

    # Calculate accuracy
    accuracy = np.mean(y_pred == y_true)
    print(f"Validation Accuracy: {accuracy:.4f}")

    # Confusion Matrix
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(10, 8))
    sns.heatmap(cm, annot=True, cmap='BuPu', fmt='g',
                xticklabels=labels, yticklabels=labels,
                linewidths=0.5, linecolor='white')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix - {title}')
    plt.show()

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred, target_names=labels))

    # Class-wise Accuracy
    print("\nClass-wise Accuracy:")
    for i, label in enumerate(labels):
        correct = np.sum((y_pred == y_true) & (y_true == i))
        total = np.sum(y_true == i)
        print(f"{label}: {correct/total:.2%}")

    return accuracy

"""# **8. Hyperparameter Tuning**"""

# Hyperparameter tuning
def hyperparameter_tuning():
    # Hyperparameter grid
    dropout_rates = [0.3, 0.5]  # AlexNet originally used 0.5
    learning_rates = [0.001, 0.0001]

    # Store results
    results = []

    for dropout in dropout_rates:
        for lr in learning_rates:
            print(f"\n\n===== Training AlexNet with dropout={dropout}, learning_rate={lr} =====")

            # Create generators
            train_generator, validation_generator = create_generators()

            # Create and compile model
            model = create_alexnet(dropout_rate=dropout, learning_rate=lr)

            # Define callbacks
            early_stopping = EarlyStopping(
                monitor='val_accuracy',
                patience=5,
                restore_best_weights=True
            )

            model_checkpoint = ModelCheckpoint(
                f'alexnet_garbage_dropout{dropout}_lr{lr}.h5',
                monitor='val_accuracy',
                save_best_only=True,
                mode='max'
            )

            # Train the model
            history = model.fit(
                train_generator,
                epochs=EPOCHS,
                validation_data=validation_generator,
                callbacks=[early_stopping, model_checkpoint],
                verbose=1
            )

            # Plot training history
            plot_training_history(history, f'AlexNet, Dropout={dropout}, LR={lr}')

            # Evaluate model
            accuracy = evaluate_model(model, validation_generator, f'AlexNet, Dropout={dropout}, LR={lr}')

            # Store results
            results.append({
                'dropout': dropout,
                'learning_rate': lr,
                'accuracy': accuracy,
                'epochs_trained': len(history.history['accuracy']),
                'val_loss': min(history.history['val_loss']),
                'model_path': f'alexnet_garbage_dropout{dropout}_lr{lr}.h5'
            })

    # Create results dataframe
    results_df = pd.DataFrame(results)
    print("\n===== AlexNet Hyperparameter Tuning Results =====")
    print(results_df)

    # Find best model
    best_model_info = results_df.loc[results_df['accuracy'].idxmax()]
    print("\n===== Best AlexNet Model =====")
    print(f"Dropout: {best_model_info['dropout']}")
    print(f"Learning Rate: {best_model_info['learning_rate']}")
    print(f"Accuracy: {best_model_info['accuracy']:.4f}")
    print(f"Epochs Trained: {best_model_info['epochs_trained']}")
    print(f"Validation Loss: {best_model_info['val_loss']:.4f}")

    # Plot all results
    plt.figure(figsize=(10, 6))
    for idx, row in results_df.iterrows():
        plt.scatter(row['learning_rate'], row['accuracy'],
                   s=100, label=f"Dropout={row['dropout']}")

    plt.xscale('log')
    plt.xlabel('Learning Rate (log scale)')
    plt.ylabel('Validation Accuracy')
    plt.title('AlexNet Hyperparameter Tuning Results')
    plt.legend()
    plt.grid(True)
    plt.show()

    # Load and return best model
    best_model = load_model(best_model_info['model_path'])
    return best_model, best_model_info

"""# **9. Run Hyperparameter Search**"""

# Run hyperparameter tuning
best_model, best_model_info = hyperparameter_tuning()

# Save final best model
best_model.save('final_alexnet_garbage_classifier.h5')
print(f"Final AlexNet model saved as 'final_alexnet_garbage_classifier.h5'")

"""# **10. Interactive Prediction Testing**"""

# Interactive prediction function
def predict_from_folder(category_idx, image_idx):
    try:
        folder = os.path.join(train_dir, labels[category_idx])
        file_list = sorted(os.listdir(folder))
        img_path = os.path.join(folder, file_list[image_idx])
        img = mpimg.imread(img_path)

        # Show Image
        plt.imshow(img)
        plt.title(f"Selected Image from '{labels[category_idx]}'")
        plt.axis('off')
        plt.show()

        # Preprocess
        test_img = tf.keras.preprocessing.image.load_img(img_path, target_size=IMAGE_SIZE)
        test_array = tf.keras.preprocessing.image.img_to_array(test_img)
        test_array = np.expand_dims(test_array, axis=0) / 255.0  # Normalize to [0,1]

        # Predict
        pred = best_model.predict(test_array)
        pred_label = labels[np.argmax(pred)]
        confidence = np.max(pred) * 100

        # Show all class probabilities
        plt.figure(figsize=(8, 4))
        plt.bar(labels, pred[0] * 100)
        plt.xlabel('Garbage Class')
        plt.ylabel('Confidence (%)')
        plt.title('Prediction Confidence for Each Class')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.show()

        print(f"\n🔍 Predicted Class: {pred_label} (Confidence: {confidence:.2f}%)")
        print(f"Actual Class: {labels[category_idx]}")

        # Show confusion if misclassified
        if pred_label != labels[category_idx]:
            print(f"⚠️ Misclassification: Model confused {labels[category_idx]} with {pred_label}")

    except Exception as e:
        print(f"⚠️ Error: {e}")

# Interactive testing loop
def interactive_testing():
    print("\n===== Interactive AlexNet Model Testing =====")
    while True:
        print("\nSelect a category:")
        for i, label in enumerate(labels):
            print(f"{i}: {label}")
        try:
            cat_idx = int(input("Enter category index (0-5): "))
            folder_path = os.path.join(train_dir, labels[cat_idx])
            files = os.listdir(folder_path)
            print(f"Available images: 0 to {len(files)-1}")
            img_idx = int(input("Enter image index: "))
            predict_from_folder(cat_idx, img_idx)
        except Exception as e:
            print(f"Invalid input: {e}")

        again = input("\nDo you want to predict another? (y/n): ")
        if again.lower() != 'y':
            break

# Run interactive testing
interactive_testing()

"""# **11. Results Summary**"""

# Summary
print("\n===== AlexNet Model Training Summary =====")
print(f"Architecture: AlexNet")
print(f"Image Size: {IMAGE_SIZE}")
print(f"Batch Size: {BATCH_SIZE}")
print(f"Max Epochs: {EPOCHS}")
print(f"Best Dropout Rate: {best_model_info['dropout']}")
print(f"Best Learning Rate: {best_model_info['learning_rate']}")
print(f"Best Validation Accuracy: {best_model_info['accuracy']:.4f}")
print(f"Actual Epochs Trained: {best_model_info['epochs_trained']}")
print(f"Final Validation Loss: {best_model_info['val_loss']:.4f}")

"""# **12. Conclusions**
In this notebook, we:


*  Implemented the classic AlexNet architecture for garbage classification
*  Conducted hyperparameter tuning across dropout rates and learning rates
*   Visualized training progress and model performance
*   Created an interactive testing interface to see model predictions

The best model achieved [accuracy]% validation accuracy, demonstrating AlexNet's effectiveness for this image classification task despite being an older architecture.
"""